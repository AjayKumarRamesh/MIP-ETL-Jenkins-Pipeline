pipeline {
    agent any
    environment {
       RUBY_IMAGE = "rubytomip:${env.BUILD_NUMBER}"                               //can be used in whole pipeline
       RUBY_APP_JAR = 'com.ibm.map-RubyToMIP-2.1.jar'
    }
    stages {
        stage('GitHub Checkout') {
            steps {
                git credentialsId: 'sangita_id_rsa' ,
                url: 'ssh://git@github.ibm.com/CIO-MAP/Jenkins_Poc_mapetl.git', branch: 'master'  
            }  //steps
        } //stage

        stage('Get COS Objects') {
            environment {
                IBMCLOUD_CREDS = credentials('ibm-cloud-cr')
                IBMCLOUD_COS_CRN = 'bda9a48c-574e-4be0-b2ff-62f2d0f23ead'
                IBMCLOUD_COS_REGION = 'ap-geo'
                IBMCLOUD_COS_BUCKET = 'map-dev-01'
            }
 
            steps{
                dir("sourcecode/ruby") {
                    sh "pwd"
                    script {
                        def JENKINS_FUNC = load "./../../jenkins_files/test/JenkinsFunctions.groovy"
                        JENKINS_FUNC.cloudLogin(IBMCLOUD_CREDS_PSW, "dev", true)
                        JENKINS_FUNC.getCOSObjects(IBMCLOUD_CREDS, IBMCLOUD_COS_CRN, 
                                                   IBMCLOUD_COS_REGION, IBMCLOUD_COS_BUCKET, "RUBY_TO_MIP")
                    }

                }
            }
        }
        
        stage('Image build') {
            environment {
                IBMCLOUD_CREDS = credentials('ibm-cloud-cr')
            }

            steps {
                dir("sourcecode/ruby") {
                    sh "pwd"
                
                    //sh "ibmcloud plugin install container-registry"
                    sh "ibmcloud plugin list"
                    sh "ibmcloud login --apikey ${IBMCLOUD_CREDS_PSW} -r us-south"
                    sh "ibmcloud cr login"
                    sh "docker build -t us.icr.io/map-dev-namespace/${RUBY_IMAGE} -f spark-3.0.1-bin-hadoop2.7/Dockerfile ./spark-3.0.1-bin-hadoop2.7"
                    sh "docker images"
                    sh "docker push us.icr.io/map-dev-namespace/${RUBY_IMAGE}"
                    sh "ibmcloud cr image-list --restrict 'map-dev-namespace'"
                }
            }
        } //stage
        
        
        stage('Image deploy to Airflow') {
            environment {
                IBMCLOUD_CREDS = credentials('ibm-cloud-cr')
                DAG_ID = "RUBY_TO_MIP"
            }

            steps {
                /**
                sh "ibmcloud plugin install container-service"
                sh "ibmcloud login --apikey ${IBMCLOUD_CREDS_PSW} -r us-south"
                sh "ibmcloud ks cluster config --cluster map-dal10-16x64-01"
                sh "kubectl config current-context"
                //sh "kubectl exec -n airflow airflow-scheduler-5598f5d6ff-b6h5g -- airflow dags list"
            
                */
                script {
                    def JENKINS_FUNC = load "./jenkins_files/test/JenkinsFunctions.groovy"
                    JENKINS_FUNC.cloudLogin(IBMCLOUD_CREDS_PSW, "dev", false)
                    AIRFLOW_POD = sh(script:'kubectl get pods -n airflow --no-headers -o custom-columns=":metadata.name" | grep airflow-scheduler', returnStdout: true).trim()
                    //def (image, jar) = JENKINS_FUNC.getAirflowVars(AIRFLOW_POD, DAG_ID)
                    JENKINS_FUNC.setAirflowVars(AIRFLOW_POD, DAG_ID, RUBY_IMAGE, RUBY_APP_JAR)
                    def (image, jar) = JENKINS_FUNC.getAirflowVars(AIRFLOW_POD, DAG_ID)
                    sh "echo '${image} ${jar}'"
                }
                /*
                //sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow dags list"
                //sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables list"
                sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables get 'ruby_image'"
                sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables get 'ruby_app_jar'"
                sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables set ruby_image ${RUBY_IMAGE}"
                sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables set ruby_app_jar ${RUBY_APP_JAR}"
                sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables get 'ruby_image'"
                sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables get 'ruby_app_jar'"

                
                //sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow dags pause BDS_GEO_HIER"
                
                //sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow dags unpause BDS_GEO_HIER"
                */
            }
        }  //stage
    } //stages
} //pipeline

