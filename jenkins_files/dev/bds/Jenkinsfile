pipeline {
    agent any
    environment {
       BDS_GEO_HIER_IMAGE = 'cmdp-bds-geo-hier-mip:jenkins'                               //can be used in whole pipeline
       BDS_GEO_HIER_APP_JAR = 'MIP-ETL-Framework-2.1.jar'
    }
    stages {
        stage('GitHub Checkout') {
            steps {
                git credentialsId: 'sangita_id_rsa' ,
                url: 'ssh://git@github.ibm.com/CIO-MAP/Jenkins_Poc_mapetl.git', branch: 'master'  
            }  //steps
        } //stage

        stage('Flare ETL Framework download'){
            environment {
                GITHUB_API_TOKEN = credentials('github_api_token')
                //GITHUB_ETL_URL = "https://github.ibm.com/api/v3/repos/CIO-Mkt-DataEng/Flarelets-Developer/releases/assets/608129"
                GITHUB_ETL_URL = "https://github.ibm.com/api/v3/repos/CIO-Mkt-DataEng/Flare/releases/assets/577917"
                OUTPUT_FILENAME = "groupId-Flare-Framework-2.1.jar"
            }
            steps{
                dir("sourcecode/bds") {
                    sh "pwd"
                
                    sh 'curl -L -H "Authorization: token $GITHUB_API_TOKEN" -H "Accept:application/octet-stream" "$GITHUB_ETL_URL" -o $OUTPUT_FILENAME'
                    sh 'ls -al'

                }
            }
        }

        stage('Spark download'){
            environment {
                SPARK_URL = "https://archive.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz"
                SPARK_FILENAME = "spark-3.0.1-bin-hadoop2.7.tgz"
            }
            steps{
                dir("sourcecode/bds") {
                    sh "pwd"
                
                    sh 'curl -L -H "Accept:application/octet-stream" "$SPARK_URL" -o $SPARK_FILENAME'
                    sh 'tar -xzf spark-3.0.1-bin-hadoop2.7.tgz'
                    sh 'ls -al spark-3.0.1-bin-hadoop2.7'
                }
            }
        }
        
        stage('Maven build') {
            steps {
                dir("sourcecode/bds") {
                    sh "pwd"
                
                    sh 'mvn -version'
                    sh 'ls -al'
                    sh 'mvn install:install-file -Dfile=groupId-Flare-Framework-2.1.jar -DgroupId=com.flare -DartifactId=base -Dversion=2.1 -Dpackaging=jar'
                    sh 'mvn clean compile package -f pom.xml'
                }
            } //steps
        } //stage

        stage('Move files') {
            environment {
                IBMCLOUD_CREDS = credentials('ibm-cloud-cr')
                IBMCLOUD_COS_CRN = 'bda9a48c-574e-4be0-b2ff-62f2d0f23ead'
                IBMCLOUD_COS_REGION = 'ap-geo'
                IBMCLOUD_COS_BUCKET = 'map-dev-01'
            }
 
            steps{
                dir("sourcecode/ruby") {
                    sh "pwd"
                
                    //Prepare IBMCLOUD COS access
                    sh "ibmcloud plugin install cloud-object-storage"
                    sh "ibmcloud login --apikey ${IBMCLOUD_CREDS_PSW} -r us-south"
                    sh "ibmcloud cos config region --region=${IBMCLOUD_COS_REGION}"
                    sh "ibmcloud cos config crn --crn=${IBMCLOUD_COS_CRN}"
                    //Create folder and download files from COS to spark-3.0.1-bin-hadoop2.7/cert
                    //Keystore files are uploaded to map-dev-01 bucket with jenkins-poc/ prefix
                    //sh "mkdir spark-3.0.1-bin-hadoop2.7/cert"
                    //sh "ibmcloud cos object-get --bucket ${IBMCLOUD_COS_BUCKET} --key 'map_project_files/Ruby/cert/digikeystore.jks' spark-3.0.1-bin-hadoop2.7/cert/digikeystore.jks"
                    //sh "ibmcloud cos object-get --bucket ${IBMCLOUD_COS_BUCKET} --key 'map_project_files/Ruby/cert/javacerts.jks' spark-3.0.1-bin-hadoop2.7/cert/javacerts.jks"
                    //sh 'ls -al spark-3.0.1-bin-hadoop2.7/cert/'
                    //Copy db2jcc4.jar zip to spark jars folder
                    sh "ibmcloud cos object-get --bucket ${IBMCLOUD_COS_BUCKET} --key 'map_project_files/db2_db2driver_for_jdbc_sqlj.zip' ./db2_db2driver_for_jdbc_sqlj.zip"
                    sh "unzip db2_db2driver_for_jdbc_sqlj.zip"
                    sh "cp db2jcc4.jar spark-3.0.1-bin-hadoop2.7/jars"
                    //Copy the Dockerfile to spark-3.0.1-bin-hadoop2.7
                    sh 'cp Dockerfile spark-3.0.1-bin-hadoop2.7'
                    //Copy Maven artifacts to Spark jars folder
                    sh 'cp -r target/. spark-3.0.1-bin-hadoop2.7/examples/jars'

                }
            }
        }
        
        stage('Image build') {
            environment {
                IBMCLOUD_CREDS = credentials('ibm-cloud-cr')
            }

            steps {
                dir("sourcecode/bds") {
                    sh "pwd"
                
                    sh "ibmcloud plugin install container-registry"
                    sh "ibmcloud plugin list"
                    sh "ibmcloud login --apikey ${IBMCLOUD_CREDS_PSW} -r us-south"
                    sh "ibmcloud cr login"
                    sh "docker build -t us.icr.io/map-dev-namespace/${BDS_GEO_HIER_IMAGE} -f spark-3.0.1-bin-hadoop2.7/Dockerfile ./spark-3.0.1-bin-hadoop2.7"
                    sh "docker images"
                    //sh "docker push us.icr.io/map-dev-namespace/${BDS_GEO_HIER_IMAGE}"
                    //sh "ibmcloud cr image-list --restrict 'map-dev-namespace'"
                }
            }
        } //stage
        
        
        stage('Image deploy to Airflow') {
            environment {
                IBMCLOUD_CREDS = credentials('ibm-cloud-cr')
            }

            steps {
                sh "ibmcloud plugin install container-service"
                sh "ibmcloud login --apikey ${IBMCLOUD_CREDS_PSW} -r us-south"
                sh "ibmcloud ks cluster config --cluster map-dal10-16x64-01"
                sh "kubectl config current-context"
                //sh "kubectl exec -n airflow airflow-scheduler-5598f5d6ff-b6h5g -- airflow dags list"
            
                script {
                    env.AIRFLOW_POD = sh(script:'kubectl get pods -n airflow --no-headers -o custom-columns=":metadata.name" | grep airflow-scheduler', returnStdout: true).trim()
                }
                //sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow dags list"
                //sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables list"
                sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables get 'ruby_image'"
                sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables get 'ruby_app_jar'"
                sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables set ruby_image ${RUBY_IMAGE}"
                sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables set ruby_app_jar ${RUBY_APP_JAR}"
                sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables get 'ruby_image'"
                sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables get 'ruby_app_jar'"

                
                //sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow dags pause BDS_GEO_HIER"
                
                //sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow dags unpause BDS_GEO_HIER"
            }
        }  //stage
    } //stages
} //pipeline

