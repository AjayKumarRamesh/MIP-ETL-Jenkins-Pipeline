pipeline {
    agent { label 'kube_pod_slave' }
    environment {
       IMAGE_TAG = ''                               //can be used in whole pipeline
       APP_JAR = ''
       DAG_ID = "${DAG_NAME}"
       AUTO_RUN = env.AUTO_RUN_DAG.toBoolean()
       IBMCLOUD_CREDS = credentials('ibm-cloud-cr')
    }
    stages {
        
        stage('Request Approval') {
            steps {
                script {
                        timeout(time: 2, unit: "HOURS") {
                            emailext to: "${env.deploy_test_approvers}",
                                subject: "${env.JOB_BASE_NAME} - Build # ${env.BUILD_NUMBER} - Approval for deploy required!",
                                body: "${env.JOB_BASE_NAME} - Build # ${env.BUILD_NUMBER} - ${DAG_NAME} is going to be deployed. \n approval required: ${BUILD_URL}/input "

                            input message: "${DAG_NAME} is going to be deployed. \n Approve Deploy?", ok: 'Yes',
                            submitter: "${env.deploy_test_approvers}"
                        }
                }
            }
        }
        
        stage('Get DEV image and jar refernece') {
            environment {
                IBMCLOUD_CREDS = credentials('ibm-cloud-cr')
            }

            steps {
                script {
                    def JENKINS_FUNC = load "./jenkins_files/test/JenkinsFunctions.groovy"
                    JENKINS_FUNC.cloudLogin(IBMCLOUD_CREDS_PSW, "dev", true)
                    AIRFLOW_POD = sh(script:'kubectl get pods -n airflow --no-headers -o custom-columns=":metadata.name" | grep airflow-scheduler', returnStdout: true).trim()
                    def (image, jar) = JENKINS_FUNC.getAirflowVars(AIRFLOW_POD, DAG_ID)
                    IMAGE_TAG = image
                    APP_JAR = jar
                }
            }
        }
        stage('Deploy Image to Airflow') {
            environment {
                IBMCLOUD_CREDS = credentials('ibm-cloud-cr')
            }
            steps {
                script {
                    def JENKINS_FUNC = load "./jenkins_files/test/JenkinsFunctions.groovy"
                    JENKINS_FUNC.cloudLogin(IBMCLOUD_CREDS_PSW, "test", false)
                    AIRFLOW_POD = sh(script:'kubectl get pods -n airflow --no-headers -o custom-columns=":metadata.name" | grep airflow-scheduler', returnStdout: true).trim()
                    JENKINS_FUNC.checkDagStatus(AIRFLOW_POD, DAG_ID, true)

                    sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow dags pause ${DAG_ID}"
                    JENKINS_FUNC.moveImage(IMAGE_TAG, "map-dev", "mip-test")
                    JENKINS_FUNC.getAirflowVars(AIRFLOW_POD, DAG_ID)
                    JENKINS_FUNC.setAirflowVars(AIRFLOW_POD, DAG_ID, IMAGE_TAG, APP_JAR)
                    JENKINS_FUNC.getAirflowVars(AIRFLOW_POD, DAG_ID)
                }
            }
        }
        stage('Validate DAG') {
            environment {
                IBMCLOUD_CREDS = credentials('ibm-cloud-cr')
            }
            steps {
                script {
                    def JENKINS_FUNC = load "./jenkins_files/test/JenkinsFunctions.groovy"
                    JENKINS_FUNC.cloudLogin(IBMCLOUD_CREDS_PSW, "test", false)
                    AIRFLOW_POD = sh(script:'kubectl get pods -n airflow --no-headers -o custom-columns=":metadata.name" | grep airflow-scheduler', returnStdout: true).trim()
                    if (AUTO_RUN_DAG.toBoolean()) {
                    sh "sh echo 'Autorun set to true, triggering DAG."
                    sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow dags trigger ${DAG_ID}"
                    sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow dags unpause ${DAG_ID}"
                    JENKINS_FUNC.checkDagStatus(AIRFLOW_POD, DAG_ID, false)
                    } else {
                        sh "echo 'Autorun not selected, skipping DAG Validation'"
                        sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow dags unpause ${DAG_ID}"
                    }
                }
            }
        }
    }
    post {
        always {
        emailext to: "${env.deploy_test_approvers}",
            subject: "${env.JOB_BASE_NAME} - Build # ${env.BUILD_NUMBER} - ${currentBuild.currentResult}!",
            body: "${env.JOB_BASE_NAME} - Build # ${env.BUILD_NUMBER} - ${currentBuild.currentResult}: Check console output at ${env.BUILD_URL} to view the results."

        }

        failure {

            script {
                def JENKINS_FUNC = load "./jenkins_files/test/JenkinsFunctions.groovy"
                JENKINS_FUNC.cloudLogin(IBMCLOUD_CREDS_PSW, "prod", false)

                AIRFLOW_POD = sh(script:'kubectl get pods -n airflow --no-headers -o custom-columns=":metadata.name" | grep airflow-scheduler', returnStdout: true).trim()

                def (image, jar) = JENKINS_FUNC.getAirflowVars(AIRFLOW_POD, DAG_ID)
                STABLE_IMAGE = image
                STABLE_JAR = jar


                sh "ibmcloud ks cluster config --cluster map-dal10-16x64-02"
                sh "kubectl config current-context"

                AIRFLOW_POD = sh(script:'kubectl get pods -n airflow --no-headers -o custom-columns=":metadata.name" | grep airflow-scheduler', returnStdout: true).trim()
                JENKINS_FUNC.setAirflowVars(AIRFLOW_POD, DAG_ID, STABLE_IMAGE, STABLE_JAR)
                JENKINS_FUNC.getAirflowVars(AIRFLOW_POD, DAG_ID)
            }
        }
    }
}