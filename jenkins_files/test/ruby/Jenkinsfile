pipeline {
    agent any
    environment {
       RUBY_IMAGE = ''                               //can be used in whole pipeline
       RUBY_APP_JAR = ''
       RUBY_DAG_ID = 'RUBY_TO_MIP'
    }
    stages {
        /*
        stage('Request Approval') {
            steps {
                script {
                        timeout(time: 2, unit: "HOURS") {
                            emailext to: 'timothy.figgins1@ibm.com', 
                                subject: "${env.PROJECT_NAME} - Build # ${env.BUILD_NUMBER} - Approve deploy required!",
                                body: "${env.PROJECT_NAME} - Build # ${env.BUILD_NUMBER} - The message here must include more information about what is going to be deployed \n approve required: ${BUILD_URL}/input "
                            
                            input message: 'The message here must include more information about what is going to be deployed\n Approve Deploy?', ok: 'Yes',
                                submitter: 'timothy.figgins1@ibm.com'
                        }
                }
            }
        }
        stage('Get DEV image and jar refernece') {
            environment {
                IBMCLOUD_CREDS = credentials('ibm-cloud-cr')
            }

            steps {
                sh "ibmcloud config --check-version=false"
                sh "ibmcloud plugin install container-service"
                sh "ibmcloud login --apikey ${IBMCLOUD_CREDS_PSW} -r us-south"
                sh "ibmcloud ks cluster config --cluster map-dal10-16x64-01"
                sh "kubectl config current-context"
                //sh "kubectl exec -n airflow airflow-scheduler-5598f5d6ff-b6h5g -- airflow dags list"
            
                script {
                    env.AIRFLOW_POD = sh(script:'kubectl get pods -n airflow --no-headers -o custom-columns=":metadata.name" | grep airflow-scheduler', returnStdout: true).trim()
                    RUBY_IMAGE = sh(script:"kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables get 'ruby_image'", returnStdout: true).trim()
                    RUBY_APP_JAR = sh(script:"kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables get 'ruby_app_jar'", returnStdout: true).trim()
                }
                //sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow dags list"
                //sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables list"
            }
        }
        stage('Tag and Push Image') {
            environment {
                IBMCLOUD_CREDS = credentials('ibm-cloud-cr')
            }
            steps {
                sh "ibmcloud config --check-version=false"
                sh "ibmcloud plugin install container-registry"
                sh "ibmcloud plugin list"
                sh "ibmcloud login --apikey ${IBMCLOUD_CREDS_PSW} -r us-south"
                sh "ibmcloud cr login"
                sh "docker pull us.icr.io/map-dev-namespace/${RUBY_IMAGE}"
                sh "docker tag us.icr.io/map-dev-namespace/${RUBY_IMAGE} us.icr.io/mip-test-namespace/${RUBY_IMAGE}"
                sh "docker push us.icr.io/mip-test-namespace/${RUBY_IMAGE}"
            }
        }
        */
        stage('Deploy Image to Airflow') {
            environment {
                DAG_EXECUTION_DATE = ''
                DAG_CURRENT_RUN = ''
                DAG_STATUS = 'running'
                IBMCLOUD_CREDS = credentials('ibm-cloud-cr')
            }

            steps {
                sh "ibmcloud config --check-version=false"
                sh "ibmcloud plugin install container-service"
                sh "ibmcloud login --apikey ${IBMCLOUD_CREDS_PSW} -r us-south"
                sh "ibmcloud ks cluster config --cluster map-dal10-16x64-02"
                sh "kubectl config current-context"
                //sh "kubectl exec -n airflow airflow-scheduler-5598f5d6ff-b6h5g -- airflow dags list"
            
                script {
                    env.AIRFLOW_POD = sh(script:'kubectl get pods -n airflow --no-headers -o custom-columns=":metadata.name" | grep airflow-scheduler', returnStdout: true).trim()
                    DAG_CURRENT_RUN = sh(script:"kubectl exec -n airflow ${AIRFLOW_POD} -- airflow dags list-runs -d ${RUBY_DAG_ID} --state running -o json", returnStdout: true).trim()
                }
                //sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow dags list"
                //sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables list"

                script {
                            timeout(time: 2, unit: "HOURS") {
                                def dag_run_object = readJSON text: DAG_CURRENT_RUN
                                if (dag_run_object.size() > 0) {
                                    //def dag_run_object = readJSON text: DAG_CURRENT_RUN
                                    DAG_EXECUTION_DATE = dag_run_object[0]['execution_date']
                                    while (DAG_STATUS == 'running') {
                                        DAG_STATUS = sh(script:"kubectl exec -n airflow ${AIRFLOW_POD} -- airflow dags state ${RUBY_DAG_ID} ${DAG_EXECUTION_DATE} | grep running", returnStdout: true).trim()
                                        sh "echo 'Checking DAG status before pause: ${DAG_STATUS}'"
                                        sleep(10)
                                    }
                                    sh "echo DAG no longer in run state, continuing with deployment."
                                } else {
                                    sh "echo 'No current ${RUBY_DAG_ID} DAGs found running.'"
                                }
                            }
                }
                

                sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow dags pause ${RUBY_DAG_ID}"

                /*
                sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables get 'ruby_image'"
                sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables get 'ruby_app_jar'"
                sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables set ruby_image ${RUBY_IMAGE}"
                sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables set ruby_app_jar ${RUBY_APP_JAR}"
                sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables get 'ruby_image'"
                sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow variables get 'ruby_app_jar'"
                */
            }
        }
        stage('Validate DAG') {
            environment {
                DAG_EXECUTION_DATE = ''
                DAG_CURRENT_RUN = ''
                DAG_STATUS = 'running'
                IBMCLOUD_CREDS = credentials('ibm-cloud-cr')
            }
            steps {
                sh "ibmcloud config --check-version=false"
                //sh "ibmcloud plugin install container-service"
                sh "ibmcloud login --apikey ${IBMCLOUD_CREDS_PSW} -r us-south"
                sh "ibmcloud ks cluster config --cluster map-dal10-16x64-02"
                sh "kubectl config current-context"
                //sh "kubectl exec -n airflow airflow-scheduler-5598f5d6ff-b6h5g -- airflow dags list"
            
                script {
                    env.AIRFLOW_POD = sh(script:'kubectl get pods -n airflow --no-headers -o custom-columns=":metadata.name" | grep airflow-scheduler', returnStdout: true).trim()
                }
                //sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow tasks clear ${RUBY_DAG_ID}"
                sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow dags trigger ${RUBY_DAG_ID}"
                sh "kubectl exec -n airflow ${AIRFLOW_POD} -- airflow dags unpause ${RUBY_DAG_ID}"
                script {
                    DAG_CURRENT_RUN = sh(script:"kubectl exec -n airflow ${AIRFLOW_POD} -- airflow dags list-runs -d ${RUBY_DAG_ID} --state running -o json", returnStdout: true).trim()
                            timeout(time: 2, unit: "HOURS") {
                                def dag_run_object = readJSON text: DAG_CURRENT_RUN
                                if (dag_run_object.size() > 0) {
                                    //def dag_run_object = readJSON text: DAG_CURRENT_RUN
                                    DAG_EXECUTION_DATE = dag_run_object[0]['execution_date']
                                    while (DAG_STATUS != 'success') {
                                        DAG_STATUS = sh(script:"kubectl exec -n airflow ${AIRFLOW_POD} -- airflow dags state ${RUBY_DAG_ID} ${DAG_EXECUTION_DATE} | grep success", returnStdout: true).trim()
                                        sh "echo 'Checking DAG status for success: ${DAG_STATUS}'"
                                        sleep(10)
                                    }
                                    sh "echo DAG no longer in run state, continuing with deployment."
                                } else {
                                    sh "echo 'No current ${RUBY_DAG_ID} DAGs found running.'"
                                }
                            }
                }
            }
        }
        stage('Notification of Completion') {
            steps {
                post {
                    always {
                    emailext to: 'timothy.figgins1@ibm.com', 
                        subject: "${env.PROJECT_NAME} - Build # ${env.BUILD_NUMBER} - ${env.BUILD_STATUS}!",
                        body: "${env.PROJECT_NAME} - Build # ${env.BUILD_NUMBER} - ${env.BUILD_STATUS}: Check console output at ${env.BUILD_URL} to view the results."
            
                    } //always
                } //post
            }
        }
    }
}