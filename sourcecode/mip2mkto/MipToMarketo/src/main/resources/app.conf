{
    ### Mandatory properties. Cannot have null or empty values ##########
    # secrets in use. Example is for kubernetes
    secrets {
        ImplementationType = k8sSecret
    }
    # points to the spark configuration file
    sparkConfig {
        file = spark-conf.properties
    }
    # points to the kafka configuration file
    kafkaConfig{
        file = kafka-dev.properties
    }
    kafkaConfigDev{
      file = cmdp-kafka-dev.properties
    }
    # DB connection info to access application configuration
    # It also provides the secrets implementation to use (e.g k8sSecret)
    base_db {
        db_type = DB2
        jdbc_url = k8sSecret.BASE_DB_ENDPOINT
        user_id = k8sSecret.BASE_DB_USER_ID
        password = k8sSecret.BASE_DB_PASSWORD
        #trust_path = k8sSecret.BASE_DB_TRUSTSTORE_PATH # Optional relative path to the application
        #trust_password = k8sSecret.BASE_DB_TRUSTSTORE_PASSWORD #Optional trust store password
    }
    # The configuration data set to read data source and sink access details
    data_sources {
        # db (support for other source types, like json in future releases)
        type=db
        # data sources (if 'db' set in <schema>.<tableName> format)
        source=MAP_ETL.ETL_DATA_SOURCES
        # repository to SQLs as data source. elt_data_sources link to this data
        sqls_source = MAP_ETL.ETL_JOB_SQL # Not used by Framework code
    }
    # The repository to store/log Job run information
    job_history {
        # db (support for other data destinations in future releases)
        type = db
        # if 'db' set in <schema>.<tableName> format
        source = MAP_ETL.ETL_JOB_HIST
        ##source = MAP_ETL.ETL_JOB_HIST_V1
    }

    ########## Include optional properties ##################
    optional {
       db_audit_columns {
            create_user_id=CREATE_ID
            update_user_id=UPDATE_ID
            create_timestamp=CREATE_TS
            update_timestamp=UPDATE_TS
            row_status_code=ROW_STATUS_CD
        }
        # Data processing defaults while processing data thru Spark & JDBC
        data_read_write {
           partition_count=20
           default_partition_lower_bound=1
           default_write_batch_size=50000
           default_read_batch_size=10000
        }

       # CSV options while reading and/or writing the files
       csv_options {
        delimiter = ;
        # Default value for quote is " (single quote) in this you don't need to set data_read_write
        # Set only if it is other than single quote.
        # quote =
        escape = "\\"
        multiLine = true
        inferSchema = true
      }
    }
}