{
    ### Mandatory properties. Cannot have null or empty values ##########
    secrets {
    # secrets in use. Example is for kubernetes
        ImplementationType = k8sSecret
    }
    # points to the spark configuration file
    sparkConfig {
        file = spark-conf.properties
    }
    # points to the kafka configuration file
    kafkaConfig{
        file = kafka-dev.properties
    }
    kafkaConfigDev{
      file = cmdp-kafka-dev.properties
    }
    # DB connection info to access application configuration
    # It also provides the secrets implementation to use (e.g k8sSecret)
    base_db {
        # !! Change db_type if needed !!
        # db_type example is for DB2
        db_type = DB2
        jdbc_url = k8sSecret.BASE_DB_ENDPOINT
        user_id = k8sSecret.BASE_DB_USER_ID
        password = k8sSecret.BASE_DB_PASSWORD
        #trust_path = /cert/javacerts.jks # Optional relative path to the application
        #trust_password = k8sSecret.BASE_DB_TRUSTSTORE_PASSWORD #Optional trust store password
    }
    # The configuration data set to read data source and sink access details
    data_sources {
        # !!! Changes needed in the entire section here !!!
        type=db
        # data sources (if db give schema.table name, if json give path to file)
        # MANDATORY TABLE mentioned in requirements
        source=MAP_ETL.ETL_DATA_SOURCES
        # repository to SQLs as data source. elt_data_sources link to this data
        # OPTIONAL TABLE mentioned in requirements
        sqls_source = MAP_ETL.ETL_JOB_SQL # Not used by Framework code
        #Job sequence stage config tableName
        job_seq_config = MAP_ETL.JOB_SEQUENCE_CONFIG
        #CDC Metadata Caching table
        job_seq_cdc_cache_table = MAP_ETL.JOB_SEQ_SQL_CACHE
            }
            # The Job Sequence configuration
        job_sequences {
                # db (support for other source types, like json in future releases)
         type=db
                # source (if 'db' set in <schema>.<tableName> format)
                #source= FRMWRK_CFG.MIGRATION_JOB_SEQ
         source= MAP_ETL.JOB_SEQUENCE_CONFIG
                     }


    # The repository to store/log Job run information
    job_history {
        # !!! Changes needed here !!!
        # db or support for other data destinations
        type = db
        # MANDATORY TABLE mentioned in requirements
        source = MAP_ETL.ETL_JOB_HIST_V1
    }

    ########## Include optional properties ##################
    optional {
       data_read_write {
           partition_count=20
           default_partition_lower_bound=1
           default_write_batch_size=50000
           default_read_batch_size=10000
        }

       # CSV options for read and write
       csv_options {
        delimiter = ;
        # Default value for quote is " (single quote) in this you don't need to set data_read_write
        # Set only if it is other than single quote.
        # quote =
        escape = "\\"
        multiLine = true
        inferSchema = true
      }
       # COS details to store job log and CRUD Error files
#          job_log_cos_info {
#          data_source_code=COS_TEST
#          bucket_name=map-test-01
#          }

    }
}