{
    ### Mandatory properties. Cannot have null or empty values ##########
    secrets {
    # secrets in use. Example is for kubernetes
        ImplementationType = k8sSecret
    }
    # points to the spark configuration file
    sparkConfig {
        file = spark-conf.properties
    }
    # points to the kafka configuration file
    kafkaConfig{
        file = kafka-dev.properties
    }
    kafkaConfigDev{
      file = cmdp-kafka-dev.properties
    }
    # DB connection info to access application configuration
    # It also provides the secrets implementation to use (e.g k8sSecret)
    base_db {
        # !! Change db_type if needed !!
        # db_type example is for DB2
        db_type = DB2
        jdbc_url = k8sSecret.BASE_DB_ENDPOINT
        user_id = k8sSecret.BASE_DB_USER_ID
        password = k8sSecret.BASE_DB_PASSWORD
        # trust_path = /cert/digikeystore.jks # Optional relative path to the application
        # trust_password = k8sSecret.BASE_DB_TRUSTSTORE_PASSWORD #Optional trust store password
    }
    # The configuration data set to read data source and sink access details
    data_sources {
        # !!! Changes needed in the entire section here !!!
        type=db
        # data sources (if db give schema.table name, if json give path to file)
        # MANDATORY TABLE mentioned in requirements
        source=MAP_ETL.ETL_DATA_SOURCES
        # repository to SQLs as data source. elt_data_sources link to this data
        # OPTIONAL TABLE mentioned in requirements
        sqls_source = MAP_ETL.ETL_JOB_SQL # Not used by Framework code
    }
    # The repository to store/log Job run information
    job_history {
        # !!! Changes needed here !!!
        # db or support for other data destinations
        type = db
        # MANDATORY TABLE mentioned in requirements
        source = MAP_ETL.ETL_JOB_HIST
    }

    ########## Include optional properties ##################
    optional {
       data_read_write {
           partition_count=20
           default_partition_lower_bound=0
           default_write_batch_size=10000
           default_read_batch_size=50000
        }

       # CSV options for read and write
       csv_options {
        delimiter = ;
        # Default value for quote is " (single quote) in this you don't need to set data_read_write
        # Set only if it is other than single quote.
        # quote =
        escape = "\\"
        multiLine = true
        inferSchema = true
      }

    # COS details to store job log and CRUD Error files
#     job_log_cos_info {
#         data_source_code=COS_PROD
#         bucket_name=map-prod-01
#     }

    }
}